---
title: Cloud Survival Solutions Rook-Ceph combined with Rainbod
description: The foundations are shaken by the mountains.Regardless of the architecture of the system, the choice of bottom storage is a topic worth exploring.
slug: ceph
image: https://static.goodrain.com/wechat/rook-ceph/ceph.png
---

The foundations are shaken by the mountains.Regardless of the architecture of the system, the choice of bottom storage is a topic worth exploring.The data contained in the business is stored and its performance directly affects the actual performance of the business application.It is precisely because of the close connection between the stored and the business data and their reliability that must be addressed, which will be a disaster level accident once the storage lapses result in the loss of business data.

## The Path to Storage Choice in the Cloud Age

In recent years, my work has been centred around the building of the Kubernetes cluster.The question of how to choose a stable and reliable storage solution for the client's Kubernetes cluster has always plagued me.

Storage can reload this basic feature requirement after Pod drift to another node, let's start by looking at the shared filesystem storage type.Nfs were first selected, and later Glusterfs were invested until recently in efforts to explore other better cloud survival solutions, a path that also gives me some idea of storage.They each have their own features：

- Nfs：Nfs are an old storage solution based on network shared files.Its strength is simple and efficient.Its shortcomings are also more pronounced, with service point failures and data not being replicated.In some scenarios where reliability is not required, Nfs remain optional.

- Glusterfs：is an open source shared storage solution.For Nfs the Gfs increase the reliability of the data by multiple copying sets, and add Brick to the mechanism so that the stock cluster extension is no longer limited to a server.The Gfs were once the first choice of my department in the production environment, guaranteeing the reliability of data by setting the reproduction factor to 3, while at the same time avoiding the problem of the data brain fracture under the distributed system.After a long period of progress, we discovered the performance shortcut of Gfs in the context of an intensive document reading and writing scene.And storage of a single shared document system type is no longer meeting our use scenario.

We have not stopped exploring the path of finding more suitable storage.These two years have been marked by a bitter concept of cloud-origin projects that are emerging in the community, including projects that store them.At first, we put our eyes on Ceph, which attracts us most from providing high-performance block type storage.However, it was persuaded at one time by its complex mode of deployment, higher transport thresholds.The emergence of the CNF Graduation Project Rook finally pulled the last threshold of contact with Ceph.

The Rook project provides a cloud survival stock programming tool that provides platform and framework level support for various types of storage, and manages the installation and operation of storage software.In version 0.9 released in 2020, Rook officially used Ceph Operator as a stable support feature for several years now.Use Rook to deploy and manage production levels Ceph cluster is still robust.

For Gfs Rook-Ceph provides extremely high performance block type storage. This is equivalent to a hard disk mounted for Pod and it is not difficult to cope with the dense file reading scene.Rook-Ceph can also provide distributive shared storage based on Cephfs and object storage based on S3 protocol in addition to providing block device type storage.The multiple storage types are managed in a unified manner and provide visualized management interfaces that are very friendly to the convoys.

As a CNF graduation, Rook-Ceph has no doubt about support for the cloud scene.The deployed Rook-Ceph cluster provides CSI plugins in the form of StorageClass to supply data volumes. It is also very friendly to all types of cloud PaaS platforms that are compatible with CSI norms.

## Rainbod and Rook

Support for the Kubernetes CSI storage interface was added in Rainbond V5.7.0-release version.

![](https://static.goodrain.com/wechat/rook-ceph/rook-ceph-1.png)

Rainbond refers to Cephfs during the installation deployment phase to deploy shared storage provided by default for all service components.For service components in a state, when adding persistent storage, you can select all the StorageClasses available in the current cluster. By selecting `root-ceph-block`, you can apply to block devices for mounting.

For how Rook-Ceph is deployed and paired with Rainbond please refer to the document [Rook-Ceph Pair] (https://www.rainbond.com/docs/ops-guide/storage/ceph-rbd "Rook-Ceph Pair")

## Use experience

This section will describe in an intuitive way the various experiences of use after Rainbond hitting Rook-Ceph storage.

### 3.1 Use shared storage

Rainbond uses Cephfs as a cluster shared storage by specifying parameters during the installation phase.In installing Rainbond with Helm, key interface parameters are set out below：

```bash
--set Cluster.RWX.enable=true \
--set Cluster.RWX.config.storageClassName=rook-cephfs \
--set Cluster.RWO.enable=true \
--set Cluster.RWO.storageClassName=rook-ceph-block
```

For any service component deployed on the Rainbond platform, it is only necessary to select the default shared storage when mounting the persistent storage, which is equivalent to keeping the data persistent in the Cepfs file system.

![](https://static.goodrain.com/wechat/rook-ceph/rook-ceph-2.png)

集群中使用组件英文名可以过滤查询所生成的 PV 资源：

```bash
$ kubectl get pv | grep mysqlcephfs
pvc-faa3e796-44c-4aa-0-b9c9-62fa0fbc8417417500Gi RWX Retain Bound guox-system/manual7-volume-mysqlcephfs-0 rainbonsssscssss2m7s
```

### 3.2 Mount block equipment

With the exception of the default shared storage, StorageClass in all other clusters is open to the state service.Manually select `rook-ceph-block` to create block device storage and mount it to Pod use.Each Pod generates a block device mount when the service component has multiple instances.

![](https://static.goodrain.com/wechat/rook-ceph/rook-ceph-3.png)

Query generated PV resource：

```bash
$kubtl get pv | grep mysql6-0
pvc-5172cb7a-cf5b-4770-afff-153c981ab09b 50Gi RWO Delete Bound guox-system/manual6-app-a710316d-mysql6-0 rook-ceph-block 5h15m
```

### 3.3 Open dashboard

Ruok-Ceph is installed on default deployment to Ceph-dashboard.Here you can monitor the entire store cluster, or change the configuration of various storage types based on graphical interface.

Modify Ceph cluster configuration, disable dashboard built-in ssl：

```bash
$ kubectl -n rook-ceeph edit hepo -n rook-ceeph Rook-eph
# modify ssl to false
spec:
  dashboard:
    enabled: true
    ssl: false

# restart operator for configuration to take effect
$ kubectl delete po -l app=rook-ceph-operator — n rook-ceeph
```

```bash
$ kubtl -n rook-ceph get service rook-ceph-mgr-dashboard
NAME TYPE CLUSTER - IP EXTERNAL-IP PORT(S) AGE
rook-ceph-mgr-dashboard ClusterIP 10.43.210.36 <none> 7000/TCP 118m 
```

Access the dashboard via gateways to get svc and use third-party component proxies on the platform, when opening an external service address.

![](https://static.goodrain.com/wechat/rook-ceph/rook-ceph-4.png)

After accessing dashboard, the default user is `admin`, get password： on the server

```bash
kubtl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath="{{['data']['password']}" | base64 --decode && echo
```

![](https://static.goodrain.com/wechat/rook-ceph/rook-ceph-5.png)

### 3.4 Object Storage

Please refer to the document [Rook-Ceph deployment pair](https://www.rainbond.com/docs/ops-guide/storage/ceph-rbd "Rook-Ceph Pair" program), which can be stored in Rook-CephJust store an object's service ClusterIP via a third-party service agent, we can get an object storage address that can be accessed by multiple clusters that are covered by the same console.Rainbond can perform cloud backup migration based on this feature.

Gets the svc address： stored by object

```bash
$ kubtl -n rook-ceph get service rook-ceph-rgw-my-store
NAME TYPE CLUSTER - IP EXTERNAL-IP PORT(S) AGE
rook-ceph-rgw-my-store ClusterIP 10.43.12.100 <none> 80/TCP 3h40m
```

![](https://static.goodrain.com/wechat/rook-ceph/rook-ceph-6.png)

This can be stored on the interface object by filling out items created in the Ceph-dashboard in the first instance by storing bucket, access-key, secret-key.

![](https://static.goodrain.com/wechat/rook-ceph/rook-ceph-7.png)

## 4. Performance Comparability Test

Using sysbench tools, we performed performance tests on Mysql using different types of storage, except data directories mounted different types of storage, and other experimental conditions are consistent.The storage types that participated in the tests included Glusterfs, Cephfs, Ceph-RBD.

The collected data are sysbench return per second of transactions (TPS) and requests per second (QPS)：

| Storage type | Mysql Memory | QPS                      | TPS                     |
| ------------ | ------------ | ------------------------ | ----------------------- |
| Glusterfs    | 1G           | 4600.22  | 230.01  |
| Cephfs       | 1G           | 18095.08 | 904.74  |
| Ceph-RBD     | 1G           | 24852.58 | 1242.62 |

The test results are obvious. Ceph blocks have the highest performance and Cepfs relative to Glusterfs have a more obvious performance advantage.

## Writing at the end

Matching the CSI container storage interface is a great feature of Rainbond v5.7.0-release version, which allows us to easily capture the excellent storage solution of Rook-Cep.With a description of the use experience of Rook-Ceph and a comparison of the final performance tests, it is necessary to say that Rook-Ceph is about to become a major offensive in our quest for cloud survival stories.
