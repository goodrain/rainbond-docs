---
title: The practice of combining cloud native storage solution Rook-Ceph with Rainbond
description: 基础不牢，地动山摇。无论是何种体系架构，底层存储的选择都是一个值得探讨的话题。
slug: ceph
image: https://static.goodrain.com/wechat/rook-ceph/ceph.png
---

基础不牢，地动山摇。无论是何种体系架构，底层存储的选择都是一个值得探讨的话题。存储承载着业务的数据，其性能直接影响到业务应用的实际表现。也正因为存储和业务的数据关联紧密，其可靠性也必须得到关注，存储的失效一旦导致业务数据丢失，那将会是一场灾难级别的事故。

## 1. The path of storage choices in the cloud-native era

In recent years, my work has always revolved around the construction of customer Kubernetes clusters.How to choose a stable and reliable storage solution with excellent performance for the customer's Kubernetes cluster has always troubled me.The most basic functional requirement that the storage volume can be remounted after the Pod is drifted to another node made me focus on the storage type of the shared file system from the beginning.I chose Nfs at the beginning, and then put it into the embrace of Glusterfs. Until recently, I started to explore other better cloud-native storage solutions. Along the way, I also have a certain understanding of various storages.They each have their own characteristics：

存储卷可以在 Pod 漂移到其他节点后重新挂载这一最基础的功能性要求，让我一开始就把目光放在了共享文件系统这一存储类型上。最开始选择了 Nfs，到后来又投入了 Glusterfs 的怀抱，直到最近开始努力探索其他更好的云原生存储解决方案，这一路走来也让我对各种存储有了一定的了解。它们各自有着自己的特点：

- Nfs：Nfs is an old-fashioned storage solution based on network sharing files.Its advantage is simplicity and efficiency.Its shortcomings are also more obvious, the server has a single point of failure, and there is no replication mechanism for data.In some scenarios that do not require high reliability, Nfs is still the best choice.它的优点是简单高效。它的缺点也比较明显，服务端单点故障，数据没有复制机制。在某些对可靠性要求不高的场景下，Nfs依然是不二之选。

- Glusterfs：这是一种开源的分布式共享存储解决方案。相对于 Nfs 而言，Gfs 通过多副本复制集提升了数据的可靠性，添加 Brick 的机制也让存储集群的扩展不再受限于一台服务器。Gfs 一度是我部在生产环境下的首选，通过将复制因子设置为 3 ，保障了数据的可靠性的同时，又能够避免分布式系统下的数据脑裂的问题。伴随 Gfs 一起前进了很久之后，我们也发现了它在密集小文件读写场景下的性能短板。而且单一的共享文件系统类型的存储，也渐渐不再满足我们的使用场景需要。

我们在寻找更合适的存储这一道路上一直没有停止探索。这两年云原生概念炙手可热，社区中不断涌现出来各种云原生领域项目，其中也不乏存储相关的项目。最开始，我们将目光放在 Ceph 身上，它最吸引我们的是可以提供高性能的块设备类型存储。然而被其复杂的部署方式、较高的运维门槛一度劝退。而 CNCF 毕业项目 Rook 的出现，终于铲平了接触 Ceph 的最后一道门槛。

The Rook project provides a cloud-native storage orchestration tool, provides platform-level and framework-level support for various types of storage, and manages the installation, operation and maintenance of storage software.Rook officially included Ceph Operator as a stable supported feature in version 0.9 released in 2018, and it has been several years so far.Using Rook to deploy and manage a production-level Ceph cluster is quite robust.Rook 在 2018 年发布的 0.9 版本中，正式将 Ceph Operator 作为稳定支持的特性，迄今已经数年。使用 Rook 部署和管理生产级别的 Ceph 集群还是非常稳健的。

相对于 Gfs ，Rook-Ceph 提供了性能极高的块设备类型存储，这相当于为 Pod 挂载了一块硬盘，应对密集小文件读写场景并非难事。Compared with Gfs, Rook-Ceph provides block device type storage with extremely high performance, which is equivalent to mounting a hard disk for Pod, and it is not difficult to deal with the scenario of intensive small file reading and writing.In addition to providing block device type storage, Rook-Ceph can also provide distributed shared storage based on Cephfs, and object storage based on the S3 protocol.Unified management of multiple storage types, and a visual management interface is provided, which is very friendly to operation and maintenance personnel.多种存储类型统一管理，并提供了可视化管理界面，对于运维人员非常友好。

作为 CNCF 毕业项目，Rook-Ceph 对云原生场景的支持毋庸置疑。As a CNCF graduate project, Rook-Ceph's support for cloud-native scenarios is beyond doubt.The deployed Rook-Ceph cluster provides a CSI plug-in, which provides data volumes to Kubernetes in the form of StorageClass, and is also very friendly to various cloud-native PaaS platforms that are compatible with the CSI specification.

## 2. The connection between Rainbond and Rook

In Rainbond V5.7.0-release, support for the Kubernetes CSI container storage interface was added.

![](https://static.goodrain.com/wechat/rook-ceph/rook-ceph-1.png)

Our search for more suitable storage has not stopped.In the past two years, the concept of cloud native has become very popular, and various cloud native projects have emerged in the community, including storage-related projects.At first, we focused on Ceph, which attracted us most because of its high-performance block device type storage.However, it was once persuaded by its complex deployment method and high operation and maintenance threshold.The emergence of Rook, the CNCF graduation project, finally leveled the last threshold for contacting Ceph.During the installation and deployment phase of Rainbond, Cephfs will be referenced to deploy the shared storage provided by default for all service components.For stateful service components, when adding persistent storage, you can select all available StorageClasses in the current cluster, and you can apply for a block device to mount by selecting `rook-ceph-block` , and the whole process is graphically interfaced. Very convenient.

How to deploy Rook-Ceph and connect it to Rainbond, please refer to document [Rook-Ceph connection solution](https://www.rainbond.com/docs/ops-guide/storage/ceph-rbd "Rook-Ceph 对接方案").

## 3. User experience

In this chapter, I will describe the various usage experiences after Rainbond is connected to Rook-Ceph storage in an intuitive way.

### 3.1 Using shared storage

The practice of combining cloud native storage solution Rook-Ceph with RainbondRainbond connects to Cephfs as a cluster shared storage by specifying parameters during the installation phase.In the process of using Helm to install Rainbond, the key docking parameters are as follows：

```bash
--set Cluster.RWX.enable=true \
--set Cluster.RWX.config.storageClassName=rook-cephfs \
--set Cluster.RWO.enable=true \
--set Cluster.RWO.storageClassName=rook -ceph-block
```

For any service component deployed on the Rainbond platform, it is only necessary to select the default shared storage when mounting the persistent storage, which is equivalent to persistently saving the data into the Cephfs file system.

![](https://static.goodrain.com/wechat/rook-ceph/rook-ceph-2.png)

Using the English name of the component in the cluster can filter the PV resources generated by the query：

```bash
$ kubectl get pv | grep mysqlcephfs
pvc-faa3e796-44cd-4aa0-b9c9-62fa0fbc8417 500Gi RWX Retain Bound guox-system/manual7-volume-mysqlcephfs-0 rainbondsssc 2m7s
```

### 3.2 Mounting a block device

除了默认的共享存储之外，其他所有集群中的 StorageClass 都面向有状态服务开放。手动选择 `rook-ceph-block` 即可创建块设备类型存储，并挂载给 Pod 使用。Except for the default shared storage, all StorageClasses in the cluster are exposed to stateful services.Manually select `rook-ceph-block` to create block device type storage and mount it for Pod use.When the service component has multiple instances, each Pod will generate a block device mount to use.

![](https://static.goodrain.com/wechat/rook-ceph/rook-ceph-3.png)

Query the generated PV resource：

```bash
$ kubectl get pv | grep mysql6-0
pvc-5172cb7a-cf5b-4770-afff-153c981ab09b 50Gi RWO Delete Bound guox-system/manual6-app-a710316d-mysql6-0 rook-ceph-block 5h15m
```

### 3.3 Open the dashboard

Rook-Ceph 默认部署时安装了可视化操作界面 Ceph-dashboard。The visual operation interface Ceph-dashboard is installed by default when Rook-Ceph is deployed.Here, you can monitor the entire storage cluster, and you can also change the configuration of various storage types based on graphical interface operations.

Modify Ceph cluster configuration to disable dashboard built-in ssl：

```bash
$ kubectl -n rook-ceph edit cephcluster -n rook-ceph rook-ceph
# Modify ssl to false
spec:
  dashboard:
    enabled: true
    ssl: false

# Restart the operator to make the configuration take effect
$ kubectl delete po - l app=rook-ceph-operator -n rook-ceph
```

```bash
$ kubectl -n rook-ceph get service rook-ceph-mgr-dashboard
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
rook-ceph-mgr-dashboard ClusterIP 10.43.210.36 <none> 7000/TCP 118m 
```

Obtain svc, use third-party components as an agent on the platform, and open the external service address to access the dashboard through the gateway.

![](https://static.goodrain.com/wechat/rook-ceph/rook-ceph-4.png)

After accessing the dashboard, the default user is`admin`, execute the following command on the server to obtain the password：

```bash
kubectl -n rook-ceph get secret rook-ceph-dashboard-password -o jsonpath="{['data']['password']}" | base64 --decode && echo
```

![](https://static.goodrain.com/wechat/rook-ceph/rook-ceph-5.png)

### 3.4 Using Object Storage

Please refer to document [Rook-Ceph deployment interface](https://www.rainbond.com/docs/ops-guide/storage/ceph-rbd "Rook-Ceph 对接方案") , you can deploy object storage in Rook-Ceph.By simply passing the service ClusterIP of the object storage through a third-party service proxy, we can obtain an object storage address that can be accessed by multiple clusters managed by the same console at the same time.Rainbond can implement cloud backup and migration based on this feature.只需要将对象存储的 service ClusterIP 通过第三方服务代理，我们就可以得到一个可以被同个控制台纳管的多个集群同时访问的对象存储地址。Rainbond 可以基于这一特性，实现云端备份迁移功能。

Get the svc address：of the object store

```bash
$ kubectl -n rook-ceph get service rook-ceph-rgw-my-store
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
rook-ceph-rgw-my-store ClusterIP 10.43.12.100 <none> 80/TCP 3h40m
```

![](https://static.goodrain.com/wechat/rook-ceph/rook-ceph-6.png)

By filling in the object storage bucket, access-key, and secret-key created in Ceph-dashboard in advance in the enterprise settings, you can connect to the object storage.

![](https://static.goodrain.com/wechat/rook-ceph/rook-ceph-7.png)

## 4. Performance comparison test

We use the sysbench tool to test the performance of Mysql using different types of storage. Except that the data directory is mounted with different types of storage, other experimental conditions are the same.The storage types participating in the test include Glusterfs, Cephfs, and Ceph-RBD.参与测试的存储类型包括 Glusterfs、Cephfs、Ceph-RBD 三种。

The data collected are transactions per second (TPS) and requests per second (QPS) returned by the sysbench test：

| storage type | Mysql memory | QPS                      | TPS                     |
| ------------ | ------------ | ------------------------ | ----------------------- |
| Glusterfs    | 1G           | 4600.22  | 230.01  |
| Cephfs       | 1G           | 18095.08 | 904.74  |
| Ceph-RBD     | 1G           | 24852.58 | 1242.62 |

The test results are obvious, the Ceph block device has the highest performance, and Cephfs also has obvious performance advantages over Glusterfs.

## 5. Write at the end

Adapting to the Kubernetes CSI container storage interface is a major feature of Rainbond v5.7.0-release. This feature allows us to easily interface with Rook-Ceph, an excellent storage solution.Through the description of the use experience of Rook-Ceph and the final performance test comparison, it has to be said that Rook-Ceph will soon become a main direction of our exploration in the field of cloud native storage.通过对 Rook-Ceph 的使用体验的描述以及最后的性能测试对比，不得不说，Rook-Ceph 即将成为我们在云原生存储领域探索的一个主攻方向。
